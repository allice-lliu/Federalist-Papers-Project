{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedPapersProject.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJN1a7AxuKYe1Xuj2jG4J0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allice-lliu/Federalist-Papers-Project/blob/main/FedPapersProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8rRAKVDagvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c10468f6-7e4c-4915-956a-fe251aab2979"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezS1JgFUiR2h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "649d06f9-4b08-4fc1-e44a-b930d3dd5c06"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "import pprint\n",
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from collections import defaultdict\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdFAFXSQeVm6"
      },
      "source": [
        "f_known = open('/content/drive/My Drive/ML Project/fedpapersKnown.txt', 'r').read().lower()\n",
        "f_contest = open('/content/drive/My Drive/ML Project/fedpapersContest.txt', 'r').read().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKfUhhY6eqmz"
      },
      "source": [
        "def makeCorpus(f):\n",
        "    # splits text into paragraphs\n",
        "    papersL = f.split(\"\\n\\n\")\n",
        "\n",
        "    # lists containing the indexes of the start and end line of each paper\n",
        "    starting_indexes = []\n",
        "    ending_indexes = []\n",
        "    for i in range(len(papersL)):\n",
        "        if papersL[i] == \"to the people of the state of new york:\":\n",
        "            starting_indexes.append(i)\n",
        "        if papersL[i] == \"publius\":\n",
        "            ending_indexes.append(i)\n",
        "\n",
        "    # prepares each paragraph\n",
        "    documents = []\n",
        "    authors = []\n",
        "    for i in range(len(starting_indexes)):\n",
        "        index = starting_indexes[i] + 1\n",
        "        doc = []\n",
        "        while index < ending_indexes[i]:\n",
        "            doc.append(papersL[index].replace(\"\\n\", \" \"))\n",
        "            index += 1\n",
        "        documents.append(' '.join(doc))\n",
        "        authors.append(papersL[starting_indexes[i] - 1])\n",
        "    return documents, authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thuZw65miobx"
      },
      "source": [
        "def process(l):\n",
        "    # make everything lowercase, remove punctuation and stopwords\n",
        "    # and only keep words that appear more than once\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    processed_list = []\n",
        "    for i in l:\n",
        "        i = i.translate(str.maketrans('', '', string.punctuation))\n",
        "        word_tokens = word_tokenize(i)\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "        processed_list.append(filtered_sentence)\n",
        "\n",
        "    frequency = defaultdict(int)\n",
        "    for text in processed_list:\n",
        "        for token in text:\n",
        "            frequency[token] += 1\n",
        "    processed_corpus = [[token for token in text if frequency[token] > 1] for text in processed_list]\n",
        "    return processed_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCL_JcUZi1K3"
      },
      "source": [
        "def get_term_weights(corpus, processed_corpus):\n",
        "    term_weights = []\n",
        "    for i in range(len(corpus)):\n",
        "        paragraph = corpus[i]\n",
        "        word_count = []\n",
        "        for j in range(len(processed_corpus[i])):\n",
        "            word_count.append(paragraph.count(processed_corpus[i][j]))\n",
        "        term_weights.append(word_count)\n",
        "    return term_weights\n",
        "\n",
        "def tfidf_weights(processed_corpus, term_weights):\n",
        "    weights = np.zeros(0)\n",
        "    words_dict = corpora.Dictionary(processed_corpus)\n",
        "    bow_corpus = [words_dict.doc2bow(text) for text in processed_corpus]\n",
        "    model = models.TfidfModel(bow_corpus)\n",
        "\n",
        "    for i in range(len(processed_corpus)):\n",
        "        tfidf = model[words_dict.doc2bow(processed_corpus[i])]\n",
        "        total = 0\n",
        "        for j in range(len(tfidf)):\n",
        "            total += term_weights[i][j] * tfidf[j][1] / len(processed_corpus[i])\n",
        "        weights = np.hstack((weights, [total]))\n",
        "    return np.vstack(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvsrcnJpjClY"
      },
      "source": [
        "def get_punctuation(corpus):\n",
        "    punct = np.zeros((len(corpus), 3))\n",
        "    for i in range(len(corpus)):\n",
        "        num_words = len(corpus[i].split())\n",
        "        # Commas per sentence\n",
        "        punct[i, 0] = corpus[i].count(',') / float(num_words)\n",
        "        # Semicolons per sentence\n",
        "        punct[i, 1] = corpus[i].count(';') / float(num_words)\n",
        "        # Colons per sentence\n",
        "        punct[i, 2] = corpus[i].count(':') / float(num_words)\n",
        "    return punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scuxwou8jGPq"
      },
      "source": [
        "# converts the list of labels to authors\n",
        "def num_to_author(labels, authors):\n",
        "    count = 0\n",
        "    a1 = 0\n",
        "    l = []    \n",
        "    for i in range(len(labels)):\n",
        "        count = count+1 if labels[i] == 0 else count-1\n",
        "\n",
        "    if count >= 0: a1 = 1 \n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        l.append('Madison') if labels[i] == a1 else l.append('Hamilton')\n",
        "    return l\n",
        "\n",
        "# converts the list of authors to labels\n",
        "def author_to_num(authors, labels):\n",
        "    count = 0\n",
        "    a1 = ''\n",
        "    l = []\n",
        "    for i in range(len(labels)):\n",
        "        count = count+1 if authors[i] == 'hamilton' else count-1\n",
        "\n",
        "    a1 = 'hamilton' if count >= 0 else 'madison'\n",
        "\n",
        "    for i in range(len(authors)):\n",
        "        l.append(0) if authors[i] == a1 else l.append(1)\n",
        "    return l\n",
        "\n",
        "\n",
        "def get_accuracy(authors, labels):\n",
        "    count = 0\n",
        "    for i in range(len(authors)):\n",
        "        if labels[i] == authors[i]:\n",
        "            count += 1\n",
        "    return count / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RECZelBwjHZb"
      },
      "source": [
        "known_corpus, known_authors = makeCorpus(f_known)\n",
        "processed_known_corpus = process(known_corpus)\n",
        "known_term_weights = get_term_weights(known_corpus, processed_known_corpus)\n",
        "known_punct = get_punctuation(known_corpus)\n",
        "\n",
        "contest_corpus, contest_authors = makeCorpus(f_contest)\n",
        "processed_contest_corpus = process(contest_corpus)\n",
        "contest_term_weights = get_term_weights(contest_corpus, processed_contest_corpus)\n",
        "contest_punct = get_punctuation(contest_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acl6KBejdTn"
      },
      "source": [
        "# training our tfidf models\n",
        "known_tfidf_weights = tfidf_weights(processed_known_corpus, known_term_weights)\n",
        "contest_tfidf_weights = tfidf_weights(processed_contest_corpus, contest_term_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLIuueDjnHA"
      },
      "source": [
        "# make matrix\n",
        "known_matrix = np.concatenate((known_tfidf_weights, known_punct), axis=1)\n",
        "contest_matrix = np.concatenate((contest_tfidf_weights, contest_punct), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SswgbEUIjpum",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "181a59f7-ec6a-44c9-f89a-abcfd9949f00"
      },
      "source": [
        "# train KMeans model\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(known_matrix)\n",
        "labels = kmeans.labels_\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 1\n",
            " 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug3nW7BsjxiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b45b0147-db34-42c6-c1d3-d2940e965a81"
      },
      "source": [
        "labels_predict = kmeans.predict(contest_matrix)\n",
        "authors_predict = num_to_author(kmeans.predict(contest_matrix), known_authors)\n",
        "accuracy = get_accuracy(author_to_num(known_authors, labels), labels)\n",
        "\n",
        "print(\"predicted labels:\", labels_predict)\n",
        "print(\"predicted authors:\", authors_predict)\n",
        "print(\"accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted labels: [0 0 1 0 0 0 1 1 0 1 0 0]\n",
            "predicted authors: ['Hamilton', 'Hamilton', 'Madison', 'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Madison', 'Hamilton', 'Madison', 'Hamilton', 'Hamilton']\n",
            "accuracy: 0.8153846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}